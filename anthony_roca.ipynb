{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Project 1 — Optimizing Default Model by First Payment Default\n",
        "\n",
        "**Name:** Anthony Roca  \n",
        "**Due:** Feb 18, 2026  \n",
        "**Competition:** [Kaggle – Optimizing Default Model by First Payment Default](https://www.kaggle.com/competitions/optimizingdefaultmodelbyfirstpaymentdefault/overview)\n",
        "\n",
        "---\n",
        "\n",
        "## Workflow Summary\n",
        "\n",
        "| Step | Approach |\n",
        "|------|----------|\n",
        "| **Stack** | NumPy, Pandas, Scikit-learn (plus SHAP for explainability) |\n",
        "| **EDA** | Concise style similar to `labs/eda_lab/eda_lab.ipynb` — load/inspect, missing values, types, distributions, correlations, key visualizations. Markdown narrative after each code block. |\n",
        "| **Splits** | Standard train / validation / test. |\n",
        "| **Feature selection** | L1 and L2 regularization; Elastic Net for logistic regression. Fine-tune later if needed. |\n",
        "| **Models** | Elastic Net (logistic), SVM, Random Forest, XGBoost. Fixed random seeds and **sklearn Pipelines** for valid comparison. |\n",
        "| **Parameter optimization** | Grid search; deferred to the end (time-intensive). |\n",
        "| **Class imbalance** | To be decided after EDA and data prep/cleaning. |\n",
        "| **Global explainability** | SHAP (Shapley) analysis — plots plus written interpretation. |"
      ],
      "id": "2a8bd0d1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Setup & Imports"
      ],
      "id": "93bc5101"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import pipeline, model_selection, preprocessing, linear_model, svm, ensemble\n",
        "# XGBoost and SHAP added when needed\n",
        "RANDOM_STATE = 42"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "02641145"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Exploratory Data Analysis (EDA)\n",
        "\n",
        "Load and inspect the dataset; summarize missing values, feature types, distributions, correlations; add concise visualizations. Narrative in markdown after each code block."
      ],
      "id": "c6902fb5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Train / Validation / Test Split\n",
        "\n",
        "Standard split; document method and proportions."
      ],
      "id": "570321d8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Feature Selection\n",
        "\n",
        "L1/L2 regularization; Elastic Net for logistic regression. Refine later if needed."
      ],
      "id": "3de40c6c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Train Multiple Classification Models\n",
        "\n",
        "Elastic Net, SVM, Random Forest, XGBoost — using fixed `RANDOM_STATE` and sklearn Pipelines. Compare on validation set."
      ],
      "id": "5808f775"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}